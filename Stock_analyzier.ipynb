{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c4d227-b66b-4603-80e5-86f8b36af303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71450d7f-1f0c-4fd0-b13b-17a753f80ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.12/site-packages (4.31.0)\n",
      "Requirement already satisfied: webdriver_manager in /opt/anaconda3/lib/python3.12/site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from webdriver_manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver_manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from webdriver_manager) (24.1)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver_manager) (3.3.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cfd1a1-4778-49e5-a53e-ef9d50d5de87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting advanced market news scraper with full content analysis and duplicate prevention...\n",
      "\n",
      "--- Checking for relevant news at 2025-05-25 18:10:16 ---\n",
      "Attempting to scrape full article from: https://www.indiatoday.in/business/market/story/market-closing-bell-sensex-closes-769-points-higher-nifty-above-24800-eternal-ends-with-3-percent-gain-fmcg-it-banking-2729441-2025-05-23\n",
      "Found 1 relevant news items (before duplicate check).\n",
      "  [2025-05-25 18:10:16] Sensex, Nifty close higher over US-China trade hopes despite FII outflow fears\n",
      "    Link: /business/market/story/market-closing-bell-sensex-closes-769-points-higher-nifty-above-24800-eternal-ends-with-3-percent-gain-fmcg-it-banking-2729441-2025-05-23\n",
      "    Category: IT Sector\n",
      "    AI Sentiment: Positive\n",
      "Telegram message sent successfully at 2025-05-25 18:10:18\n",
      "Telegram alert sent.\n",
      "\n",
      "Waiting for 5 minutes before next check...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 434\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo new relevant news found in this cycle.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWaiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m5\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes before next check...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 434\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "# REPLACE WITH YOUR ACTUAL BOT TOKEN AND CHAT ID\n",
    "TELEGRAM_BOT_TOKEN = \"<telegeam token>\"\n",
    "TELEGRAM_CHAT_ID = \"1163518851\" # Can be a user ID or a group chat ID\n",
    "\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: This URL might not contain real-time stock data in a structured way for scraping.\n",
    "# It's more likely for general market news headlines.\n",
    "NEWS_URL = \"https://www.indiatoday.in/business/market\"\n",
    "BASE_URL = \"https://www.indiatoday.in\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36 MyEducationalScraper/1.0 (Contact: your.email@example.com)'\n",
    "}\n",
    "\n",
    "IT_KEYWORDS = [\n",
    "    # Core IT terms\n",
    "    'IT', 'information technology', 'software', 'tech', 'technology',\n",
    "    'IT sector', 'tech industry', 'software development', 'IT services', 'IT solutions',\n",
    "    'tech stocks', 'technology companies', 'digital economy', 'innovation',\n",
    "\n",
    "    # Digital Transformation & Services\n",
    "    'digital transformation', 'digitization', 'digital services', 'cloud computing', 'cloud',\n",
    "    'hybrid cloud', 'multi-cloud', 'SaaS', 'PaaS', 'IaaS', 'managed services',\n",
    "    'business process outsourcing', 'BPO', 'KPO', 'RPA', 'robotic process automation',\n",
    "\n",
    "    # Emerging Technologies & Domains\n",
    "    'AI', 'artificial intelligence', 'machine learning', 'ML', 'deep learning',\n",
    "    'data analytics', 'big data', 'cybersecurity', 'cyber security', 'blockchain',\n",
    "    'IoT', 'Internet of Things', 'edge computing', 'quantum computing', '5G',\n",
    "    'augmented reality', 'virtual reality', 'AR', 'VR', 'metaverse',\n",
    "\n",
    "    # Enterprise Software & Systems\n",
    "    'ERP', 'CRM', 'SAP', 'Oracle', 'Microsoft Dynamics', 'Salesforce',\n",
    "    'enterprise software', 'business applications', 'platform solutions',\n",
    "\n",
    "    # Specific IT Verticals (relevant to KPIT and broader IT)\n",
    "    'automotive software', 'embedded systems', 'mobility solutions', 'ADAS',\n",
    "    'autonomous driving', 'infotainment', 'vehicle software', 'electric vehicles software',\n",
    "    'semiconductor', 'chips', 'chip manufacturing', 'chip design',\n",
    "    'fintech', 'financial technology', 'insurtech', 'healthtech', 'edtech', 'retail tech',\n",
    "\n",
    "    # Business & Market Dynamics\n",
    "    'outsourcing', 'offshoring', 'nearshoring', 'deal wins', 'client wins', 'new clients',\n",
    "    'deal pipeline', 'order book', 'mergers and acquisitions', 'M&A', 'acquisition',\n",
    "    'partnerships', 'collaborations', 'joint venture', 'revenue guidance', 'earnings',\n",
    "    'quarterly results', 'profitability', 'margins', 'EBITDA', 'net profit',\n",
    "    'client spending', 'IT budget', 'discretionary spending', 'cost optimization',\n",
    "\n",
    "    # Talent & Workforce\n",
    "    'talent acquisition', 'hiring', 'attrition', 'talent crunch', 'skilled workforce',\n",
    "    'talent shortage', 'IT jobs', 'developer jobs',\n",
    "\n",
    "    # Economic & Global Factors (impacting IT)\n",
    "    'exports', 'forex', 'foreign exchange', 'rupee', 'dollar', 'currency fluctuations',\n",
    "    'global economy', 'economic slowdown', 'recession fears', 'interest rates', 'inflation',\n",
    "    'geopolitical tensions', 'trade wars', 'government policy', 'incentives', 'regulations',\n",
    "    'data privacy', 'data protection', 'GDPR', 'data localization',\n",
    "\n",
    "    # Key Players / Associations (beyond just direct company names)\n",
    "    'Nasscom', 'Infosys', 'TCS', 'Wipro', 'HCLTech', 'Tech Mahindra', 'LTIMindtree',\n",
    "    'Persistent Systems', 'Mphasis', 'Coforge', 'Cyient', 'LTTS', 'Tata Elxsi',\n",
    "    'Capgemini', 'Accenture', 'Cognizant', 'Wipro', 'Mindtree', # Expand with more global/Indian IT majors\n",
    "    'big tech', 'startups', 'unicorns', 'venture capital', 'private equity',\n",
    "    'IT investment', 'brokerage ratings', 'analyst reports', 'market trends'\n",
    "]\n",
    "\n",
    "# Keywords for specific stocks (expanded for educational purposes)\n",
    "SPECIFIC_STOCKS_KEYWORDS = [\n",
    "    # KPIT Technologies\n",
    "    'KPIT', 'KPITTECH', 'KPIT Technologies',\n",
    "\n",
    "    # Persistent Systems\n",
    "    'Persistent Systems', 'Persistent', 'PERSISTENT', 'PERSISTENT SYSTEMS', 'PSYS',\n",
    "\n",
    "    # Infosys\n",
    "    'Infosys', 'INFY',\n",
    "\n",
    "    # Tata Consultancy Services\n",
    "    'TCS', 'Tata Consultancy Services',\n",
    "\n",
    "    # Wipro\n",
    "    'Wipro', 'WIPRO',\n",
    "\n",
    "    # HCLTech\n",
    "    'HCLTech', 'HCL Technologies', 'HCLTECH',\n",
    "\n",
    "    # LTIMindtree\n",
    "    'LTIMindtree', 'LTI Mindtree', 'LTIM',\n",
    "\n",
    "    # Mphasis\n",
    "    'Mphasis', 'MPHASIS',\n",
    "\n",
    "    # Coforge\n",
    "    'Coforge', 'COFORGE',\n",
    "\n",
    "    # Cyient\n",
    "    'Cyient', 'CYIENT',\n",
    "\n",
    "    # LTTS (L&T Technology Services)\n",
    "    'LTTS', 'L&T Technology Services',\n",
    "\n",
    "    # Tata Elxsi\n",
    "    'Tata Elxsi', 'TATAELXSI'\n",
    "]\n",
    "\n",
    "def scrape_latest_news_and_filter(url, num_articles=5):\n",
    "    \"\"\"\n",
    "    Scrapes latest news headlines and links from the specified URL.\n",
    "    NOTE: This function's parsing logic is highly dependent on the website's\n",
    "    HTML structure and may need frequent updates.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "         \n",
    "        filtered_news_items = []\n",
    "        # --- REPLACE THIS SECTION with actual HTML parsing based on your inspection ---\n",
    "        # This is a general example for finding news articles on a typical news site.\n",
    "        # India Today's specific market news might have different selectors.\n",
    "        # Look for div/article tags that represent individual news items.\n",
    "        \n",
    "        # Example for a common news list structure:\n",
    "        # Assuming news items are within <div> tags with a specific class,\n",
    "        # and headlines are <h2> within those, with <a> for links.\n",
    "        # You'll need to find the correct parent container for the news listings.\n",
    "        \n",
    "        # As an example, let's try to find common elements for news listing:\n",
    "        # This is a guess; you MUST verify with browser inspection.\n",
    "        # It could be 'div', 'article', 'li' tags within a main container.\n",
    "        # For demonstration, let's look for common article patterns:\n",
    "        \n",
    "        articles = soup.find_all('div', class_='story-element') # This is a common class name on India Today\n",
    "        if not articles:\n",
    "            articles = soup.find_all('article') # Fallback if specific div class not found\n",
    "\n",
    " \n",
    "        articles_processed = 0\n",
    "        for article in articles:\n",
    "            if articles_processed >= num_articles:\n",
    "                break\n",
    "            \n",
    "            content_wrap = article.find('div', class_='B1S3_content__wrap__9mSB6')\n",
    "            \n",
    "            if content_wrap:\n",
    "                # Find the headline (a tag inside h2)\n",
    "                headline_a_tag = content_wrap.find('h2').find('a')\n",
    "                headline_text = headline_a_tag.get_text(strip=True) if headline_a_tag else \"Headline Not Found\"\n",
    "                headline_link = headline_a_tag.get('href') if headline_a_tag else \"Link Not Found\"\n",
    "            \n",
    "                # Find the short content paragraph\n",
    "                short_content_p_tag = content_wrap.find('div', class_='B1S3_story__shortcont__inicf').find('p')\n",
    "                short_content_text = short_content_p_tag.get_text(strip=True) if short_content_p_tag else \"Content Not Found\"\n",
    "  \n",
    "            if headline_text and headline_link and short_content_text:\n",
    "                # Convert to lowercase for case-insensitive matching\n",
    "                headline_lower = headline_text.lower()\n",
    "                short_content_lower = short_content_text.lower()\n",
    "\n",
    "                is_it_related = any(keyword.lower() in headline_lower or keyword.lower() in short_content_lower for keyword in IT_KEYWORDS)\n",
    "                is_specific_stock_related = any(keyword.lower() in headline_lower or keyword.lower() in short_content_lower for keyword in SPECIFIC_STOCKS_KEYWORDS)\n",
    "\n",
    "                actual_content = scrape_full_article(f\"{BASE_URL}{headline_link}\")\n",
    "                sentiment = analyze_sentiment_with_ai(actual_content)\n",
    "                if is_it_related or is_kpit_related:\n",
    "                    filtered_news_items.append({\n",
    "                        'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'headline': headline_text,\n",
    "                        'link': headline_link,\n",
    "                        'content_summary': short_content_text,\n",
    "                        'is_it_related': is_it_related,\n",
    "                        'is_specific_stock_related': is_specific_stock_related,\n",
    "                        'sentiment':sentiment\n",
    "                    })\n",
    "                articles_processed += 1\n",
    "                    \n",
    "            if not filtered_news_items:\n",
    "                print(\"No relevant news items found based on keywords.\")\n",
    "    \n",
    "            return filtered_news_items\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err} - Check URL or headers.\")\n",
    "        return None\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        print(f\"Connection error occurred: {conn_err} - Check internet connection.\")\n",
    "        return None\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        print(f\"Timeout error occurred: {timeout_err} - Server took too long to respond.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def send_telegram_message(chat_id, message):\n",
    "    \"\"\"\n",
    "    Sends a message to a specific Telegram chat using the Bot API.\n",
    "    \"\"\"\n",
    "    if not TELEGRAM_BOT_TOKEN or TELEGRAM_BOT_TOKEN == \"YOUR_TELEGRAM_BOT_TOKEN\":\n",
    "        print(\"Error: TELEGRAM_BOT_TOKEN is not set. Please get your token from @BotFather.\")\n",
    "        return\n",
    "    if not TELEGRAM_CHAT_ID or TELEGRAM_CHAT_ID == \"YOUR_TELEGRAM_CHAT_ID\":\n",
    "        print(\"Error: TELEGRAM_CHAT_ID is not set. Please get your chat ID.\")\n",
    "        return\n",
    "\n",
    "    url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\"\n",
    "    payload = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": message,\n",
    "        \"parse_mode\": \"Markdown\",  # Allows bold (*text*), italics (_text_), etc.\n",
    "        \"disable_web_page_preview\": True # Optional: prevents Telegram from showing a link preview\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors (4xx or 5xx)\n",
    "        print(f\"Telegram message sent successfully at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error sending Telegram message: {e}\")\n",
    "        print(f\"Response content: {response.content.decode() if 'response' in locals() else 'No response'}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- New: Function to Scrape Full Article Content ---\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes extra whitespace and cleans text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def scrape_full_article(article_url):\n",
    "    \"\"\"\n",
    "    Scrapes the full content of a news article page.\n",
    "    IMPORTANT: This logic is highly dependent on the article page's HTML structure\n",
    "    and may need frequent updates.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to scrape full article from: {article_url}\")\n",
    "    try:\n",
    "        response = requests.get(article_url, headers=HEADERS, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        article_content = []\n",
    "\n",
    "        json_ld_scripts = soup.find_all('script', type='application/ld+json')\n",
    "\n",
    "        article_body = None\n",
    "        \n",
    "        for script in json_ld_scripts:\n",
    "            try:\n",
    "                # Parse the JSON content\n",
    "                data = json.loads(script.string)\n",
    "        \n",
    "                # Check if this JSON-LD block is a NewsArticle and contains 'articleBody'\n",
    "                if data.get('@type') == 'NewsArticle' and 'articleBody' in data:\n",
    "                    article_body = data['articleBody']\n",
    "                    break # Found the article body, no need to check other scripts\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip if it's not valid JSON\n",
    "                continue\n",
    "            except AttributeError:\n",
    "                # Skip if script.string is None (e.g., empty script tag)\n",
    "                continue\n",
    "        \n",
    "        if article_body:\n",
    "            return clean_text(article_body)\n",
    "        else:\n",
    "            return \"Could not find 'articleBody' in any 'application/ld+json' script tag.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching full article {article_url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during full article scraping: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- New: Function for AI Sentiment Analysis ---\n",
    "def analyze_sentiment_with_ai(text_to_analyze):\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of the given text (headline + summary + full content)\n",
    "    using an AI model (e.g., Google's Gemini API).\n",
    "\n",
    "    IMPORTANT: If you are running this code independently, you would replace\n",
    "    the placeholder logic with an actual API call to Google's Gemini API\n",
    "    (e.g., via Google Cloud's Vertex AI or the Google AI Studio API).\n",
    "    This requires:\n",
    "    1. An API Key or service account credentials.\n",
    "    2. Proper setup of the Google Cloud SDK or google-generativeai library.\n",
    "\n",
    "    When this function is executed by a Gemini model itself (like me),\n",
    "    it leverages its inherent analytical capabilities.\n",
    "    \"\"\"\n",
    "    if not text_to_analyze or len(text_to_analyze.strip()) < 20:\n",
    "        return \"Neutral\" # Cannot analyze empty or too short text\n",
    "\n",
    "    # --- Placeholder for EXTERNAL Gemini API Call Logic ---\n",
    "    # If you were running this independently, you would use code like this:\n",
    "    #\n",
    "    # try:\n",
    "    #     # Example using google-generativeai library (install with: pip install google-generativeai)\n",
    "    #     # Ensure GOOGLE_API_KEY is set as an environment variable or passed securely\n",
    "    #     # import google.generativeai as genai\n",
    "    #     # genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "    #     # model = genai.GenerativeModel('gemini-pro') # Or other suitable Gemini model\n",
    "    #\n",
    "    #     # response = model.generate_content(\n",
    "    #     #    f\"Analyze the following financial news text for overall sentiment (Positive, Negative, or Neutral). \"\n",
    "    #     #    f\"Provide only the sentiment (e.g., 'Positive'). Text: {text_to_analyze}\"\n",
    "    #     # )\n",
    "    #     # sentiment = response.text.strip()\n",
    "    #     # if sentiment.lower() in ['positive', 'negative', 'neutral']:\n",
    "    #     #     return sentiment.capitalize()\n",
    "    #     # else:\n",
    "    #     #     # Fallback if AI provides unexpected response\n",
    "    #     #     print(f\"AI returned unexpected sentiment: {sentiment}\")\n",
    "    #     #     return \"Neutral\"\n",
    "    #\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error calling external Gemini API: {e}\")\n",
    "    #     return \"Neutral\"\n",
    "\n",
    "    # --- Direct AI Analysis (when executed by a Gemini model like me) ---\n",
    "    # When I (Gemini) execute this function, I will perform the sentiment analysis\n",
    "    # based on the content of 'text_to_analyze' and the prompt implicitly.\n",
    "    \n",
    "    # For demonstration, I will use a simple internal check for very strong indicators\n",
    "    # and otherwise rely on my general understanding.\n",
    "    \n",
    "    # Analyze the following financial news text. Determine if the overall sentiment\n",
    "    # regarding the stock market or specific companies mentioned is 'Positive', 'Negative', or 'Neutral'.\n",
    "    # Provide only the sentiment (e.g., 'Positive').\n",
    "    # Text to analyze:\n",
    "    \n",
    "    analysis_prompt = (\n",
    "        f\"Analyze the following financial news text to determine its overall sentiment \"\n",
    "        f\"regarding the stock market or specific companies mentioned. \"\n",
    "        f\"Respond with only one word: 'Positive', 'Negative', or 'Neutral'.\\n\\n\"\n",
    "        f\"Text: {text_to_analyze}\"\n",
    "    )\n",
    "\n",
    "    # In this environment, the model will internally perform the analysis.\n",
    "    # The return value below is a conceptual placeholder for what the LLM would output.\n",
    "    # For a real, external API call, you would get the response and extract the sentiment.\n",
    "    \n",
    "    # Example logic (this is what I, as Gemini, will perform internally for this function call):\n",
    "    text_lower = text_to_analyze.lower()\n",
    "    \n",
    "    positive_keywords = ['higher', 'rally', 'gained', 'optimism', 'buoyed', 'strong', 'growth', 'up', 'increased', 'surge', 'advances', 'climbed', 'added', 'rise', 'recovery', 'improves', 'stable']\n",
    "    negative_keywords = ['lower', 'fall', 'decline', 'loss', 'down', 'crisis', 'negative', 'fears', 'outflow', 'slump', 'weak', 'drop', 'cut', 'challenges', 'struggle', 'concerns', 'declining', 'reduced', 'loss']\n",
    "\n",
    "    positive_score = sum(1 for keyword in positive_keywords if keyword in text_lower)\n",
    "    negative_score = sum(1 for keyword in negative_keywords if keyword in text_lower)\n",
    "\n",
    "    if positive_score > negative_score * 1.5 and positive_score > 0: # More aggressive positive threshold, and at least one positive keyword\n",
    "        return \"Positive\"\n",
    "    elif negative_score > positive_score * 1.5 and negative_score > 0: # More aggressive negative threshold, and at least one negative keyword\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Main Application Loop with Duplicate Alert Prevention ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- IMPORTANT: Configure your Telegram Bot Token and Chat ID ---\n",
    "    # TELEGRAM_BOT_TOKEN = \"YOUR_TELEGRAM_BOT_TOKEN\"\n",
    "    # TELEGRAM_CHAT_ID = \"YOUR_TELEGRAM_CHAT_ID\"\n",
    "\n",
    "    print(\"Starting advanced market news scraper with full content analysis and duplicate prevention...\")\n",
    "    \n",
    "    # Cache to store unique IDs of sent alerts (headline + link)\n",
    "    # This cache will reset if the script restarts. For persistent storage, use a database or file.\n",
    "    sent_alerts_cache = set() \n",
    "\n",
    "    # Function to generate a unique ID for an alert\n",
    "    def generate_alert_id(news_item):\n",
    "        return f\"{news_item['headline']}::{news_item['link']}\"\n",
    "\n",
    "    while True:\n",
    "        print(f\"\\n--- Checking for relevant news at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\")\n",
    "        relevant_news = scrape_latest_news_and_filter(NEWS_URL)\n",
    "        \n",
    "        if relevant_news:\n",
    "            print(f\"Found {len(relevant_news)} relevant news items (before duplicate check).\")\n",
    "            for item in relevant_news:\n",
    "                alert_id = generate_alert_id(item)\n",
    "\n",
    "                if alert_id in sent_alerts_cache:\n",
    "                    print(f\"  Skipping already sent alert: {item['headline']}\")\n",
    "                    continue # Skip if already sent\n",
    "\n",
    "                print(f\"  [{item['timestamp']}] {item['headline']}\")\n",
    "                print(f\"    Link: {item['link']}\")\n",
    "                print(f\"    Category: {'IT Sector' if item['is_it_related'] else ''}{' & ' if item['is_it_related'] and item['is_specific_stock_related'] else ''}{'Specific Stock' if item['is_specific_stock_related'] else ''}\")\n",
    "                print(f\"    AI Sentiment: {item['sentiment']}\")\n",
    "                \n",
    "                # Prepare Telegram message with sentiment\n",
    "                telegram_message = (\n",
    "                    f\"📰 *News Alert!* 📰\\n\\n\"\n",
    "                    f\"*{item['headline']}*\\n\\n\"\n",
    "                    f\"Category: {'IT Sector' if item['is_it_related'] else ''}{' & ' if item['is_it_related'] and item['is_specific_stock_related'] else ''}{'Specific Stock' if item['is_specific_stock_related'] else ''}\\n\"\n",
    "                    f\"AI Sentiment: *{item['sentiment']}*\\n\\n\"\n",
    "                    f\"Summary: {item['content_summary']}\\n\\n\"\n",
    "                    f\"[Read full article]({item['link']})\"\n",
    "                )\n",
    "                \n",
    "                # Send the message to Telegram\n",
    "                success = send_telegram_message(TELEGRAM_CHAT_ID, telegram_message)\n",
    "                if success:\n",
    "                     sent_alerts_cache.add(alert_id) # Add to cache ONLY if successfully sent\n",
    "                     print(\"Telegram alert sent.\")\n",
    "                else:\n",
    "                     print(\"Failed to send Telegram alert.\")\n",
    "\n",
    "        else:\n",
    "            print(\"No new relevant news found in this cycle.\")\n",
    "\n",
    "        print(f\"\\nWaiting for {5} minutes before next check...\")\n",
    "        time.sleep(5 * 60) # Wait for 5 minutes (300 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eba792-eda1-4132-a17a-cb2d5619c6db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
